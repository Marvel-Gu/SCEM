---
title: "Section B"
author: "Dingyuan Gu"
date: "2024-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Section B

### B.1 (1)

According to the definition of probability density function:

$$
\int_{b}^{\infty} p_{\lambda } \left ( x \right ) \, dx=\int_{b}^{\infty} ae^{-\lambda \left ( x-b \right )} \,dx=1
$$

Substituting $u=x−b$: 

$$\int_{b}^{\infty} ae^{-\lambda \left ( x-b \right )} \,dx=\int_{0}^{\infty} ae^{-\lambda u} \,du=a\cdot \left [ - \frac{1}{\lambda } e^{-\lambda u} \right ]_{0}^{\infty}=a\cdot \left [ 0- \left ( - \frac{1}{\lambda }  \right )  \right ] =\frac{a}{\lambda } =1
$$

So, the value of $a$ is $\lambda$.

### B.1 (2)

#### `population mean`

According to the definition of population mean:

$$
E[X] = \int_{-\infty}^{\infty} x p_{\lambda}(x) \, dx = \int_{b}^{\infty} x a e^{-\lambda(x-b)} \, dx = \lambda\int_{b}^{\infty} xe^{-\lambda(x-b)}
$$

By partial integration:

$$
E[X] = \lambda \left[ -\frac{x}{\lambda} e^{-\lambda(x-b)} \bigg|_{b}^{\infty} + \int_{b}^{\infty} \frac{1}{\lambda} e^{-\lambda(x-b)} \, dx \right]=\lambda \left \{ \left [ 0-\left ( -\frac{b}{\lambda }  \right )  \right ] + \frac{1}{\lambda  } \left [ 0-\left ( -\frac{1}{\lambda }  \right )  \right ]   \right \} = b+\frac{1}{\lambda }
$$

So, population mean of the random variable $X$ is $b+\frac{1}{\lambda }$

####  `standard deviation`
According to the calculation formula of standard deviation: 
$$
\sigma(X) = \sqrt{\operatorname{Var}(X)} = \sqrt{E\left[X^2\right] - [E(X)]^2}
$$

We need $E[X^2]$:

$$
E[X^2] = \int_{b}^{\infty} x^2 p_{\lambda}(x) \, dx = \lambda \int_{b}^{\infty} x^2 e^{-\lambda(x-b)} \, dx
$$

Using integration by parts, we get: 
$$
E[X^2] = \frac{2}{\lambda^2}+ \frac{2b}{\lambda} +b^2
$$

So $\sigma(X) = \sqrt{E\left[X^2\right] - (E[X])^2} = \sqrt{\left(b^2 + \frac{2b}{\lambda} + \frac{2}{\lambda^2}\right) - \left(b + \frac{1}{\lambda}\right)^2}=\frac{1}{\lambda}$

### B.1 (3)

#### `cumulative distribution function`

The cumulative distribution function \( F_{\lambda}(x) \) is given by:

$$ 
F_{\lambda}(x) = \int_{-\infty}^{x} p_{\lambda}(t) \, dt 
$$

For $x < b$:

$$
F_{\lambda}(x) = 0 
$$

For $x \geq b$:

$$
F_{\lambda}(x) = \int_{b}^{x} \lambda e^{-\lambda(t-b)} \, dt= \left[ -e^{-\lambda(t-b)} \right]_{b}^{x} = 1 - e^{-\lambda(x-b)}
$$

So, cumulative distribution function is: 
$$F_{\lambda}(x) = \begin{cases} 0 & \text{if } x < b, \\1 - e^{-\lambda(x-b)} & \text{if } x \geq b.\end{cases}
$$

#### `quantile function`

For the quantile function $Q(p)$: Calculating $F_{\lambda}(x) = p$

$$
1 - e^{-\lambda(x-b)} = p \Longrightarrow e^{-\lambda(x-b)} = 1 - p $$

Taking the natural logarithm:

$$
-\lambda(x-b) = \ln(1-p) \Longrightarrow x = b - \frac{1}{\lambda}\ln(1-p)
$$

So, the quantile function is:

$$ Q(p) = b - \frac{1}{\lambda}\ln(1-p)
$$

### B.1 (4)


For independent copies $x_1, x_2, \ldots, x_n$，likelihood estimate is :

$$
L(\lambda) = \prod_{i=1}^n \left( \lambda e^{-\lambda(x_i - b)} \right) =\lambda^n e^{-\lambda \sum_{i=1}^n (x_i - b)}
$$


Taking the natural logarithm:

$$
\ln(L(\lambda)) = n \ln(\lambda) - \lambda \sum_{i=1}^n (x_i - b)
$$


Derivation:

$$
\frac{\partial \ln(L(\lambda))}{\partial \lambda}  = \frac{n}{\lambda} - \sum_{i=1}^n (x_i - b)
$$

Set the derivative equal to zero:

$$
\frac{n}{\lambda} - \sum_{i=1}^n (x_i - b) = 0
$$
$$
\frac{n}{\lambda} = \sum_{i=1}^n (x_i - b) 
$$
$$
\lambda = \frac{n}{\sum_{i=1}^n (x_i - b)}
$$


So the maximum likelihood estimate $\lambda_{\text{MLE}} = \frac{n}{\sum_{i=1}^{n}(x_i - b)}$


### B.1 (5)

```{r}
library(tidyverse)
```

```{r}
folder_path <- "C:\\Users\\levit\\Downloads\\"
file_name <- "supermarket_data_2024.csv"
file_path <- paste(folder_path,file_name,sep="")
time_df<-read_csv(file_path, show_col_types = FALSE)
time_df%>%head(5) # check

n <- nrow(time_df)
print(n)

b <- 300

time_df1<-time_df

time_df1 <- time_df1 %>%
  mutate(x_ib = TimeLength - b) %>%
  summarise(sum_x_ib = sum(x_ib))

lambda_mle <- n / time_df1$sum_x_ib
print(lambda_mle)
```
So the value of $\lambda_{\text{MLE}}$ is 0.01988426.

### B.1 (6)

```{r}
n_resamples <- 10000
lambda_mle_bootstrap <- numeric(n_resamples)

set.seed(666)

for (i in 1:n_resamples) {
  # Resampling and compute new lambda_mle
  sample <- sample(time_df$TimeLength, n, replace = TRUE)  
  lambda_mle_bootstrap[i] <- length(sample) / sum(sample-300)
}

lower_bound <- quantile(lambda_mle_bootstrap, 0.025)
upper_bound <- quantile(lambda_mle_bootstrap, 0.975)

cat("a confidence level 0f 95%: (", lower_bound, ", ", upper_bound, ")\n", sep = "")

```

### B.1 (7)

```{r}
b <- 0.01
true_lambda <- 2
sample_sizes <- seq(100, 5000, by = 10)
mse_values <- numeric(length(sample_sizes))

set.seed(666)

for (i in 1:length(sample_sizes)) {
  n <- sample_sizes[i]
  mle_estimates <- numeric(100)
  
  for (j in 1:100) {
    X <- rexp(n, rate = true_lambda) + b
    lambda_mle1 <- n / sum(X - b)
    mle_estimates[j] <- lambda_mle1
  }
  
  mse_values[i] <- mean((mle_estimates - true_lambda)^2)
}

plot(sample_sizes, mse_values, pch = 19, cex=0.5,
     xlab = "Sample Size", ylab = "Mean Squared Error (MSE)",
     main = "MSE of Maximum Likelihood Estimator")

```

### B.2 (1)

There are three possible values for X: 2, 0, -2. If we draw 2 red balls, X=2; If we draw 1 red ball and 1 blue ball, X=0; If we draw 2 blue balls, X=-2.

1. $X = 2$:
$$
P(X=2) = \frac{a}{a+b} \cdot \frac{a-1}{a+b-1}
$$
2. $X = 0$:
$$
P(X=0) = \frac{a}{a+b} \cdot \frac{b}{a+b-1} + \frac{b}{a+b} \cdot \frac{a}{a+b-1} = 2 \cdot \frac{a}{a+b} \cdot \frac{b}{a+b-1}
$$
3. $X = -2$:
$$
P(X=-2) = \frac{b}{a+b} \cdot \frac{b-1}{a+b-1}
$$

Thus, the probability mass function $p_X$ is given by:

$$ 
p_X(x) = \begin{cases} 
\frac{a(a-1)}{(a+b)(a+b-1)} & \text{if } x = 2 \\
 \frac{2ab}{(a+b)(a+b-1)} & \text{if } x = 0 \\
\frac{b(b-1)}{(a+b)(a+b-1)} & \text{if } x = -2 \\
0 & \text{otherwise}
\end{cases} 
$$

### B.2 (2)

$$E(X) = 2 \cdot P(X=2) + 0 \cdot P(X=0) - 2 \cdot P(X=-2)\\
= 2 \cdot \frac{a(a-1)}{(a+b)(a+b-1)} - 2 \cdot \frac{b(b-1)}{(a+b)(a+b-1)}\\
= \frac{2(a^2-a - b^2+b)}{(a+b)(a+b-1)}
$$


### B.2 (3)
$$
\operatorname{Var}(X) =E\left[X^2\right] - [E(X)]^2
$$

Calculating $E(X^2)$:

$$ E(X^2) = 2^2 \cdot P(X=2) + 0^2 \cdot P(X=0) + (-2)^2 \cdot P(X=-2) \\=4 \cdot \frac{a(a-1)}{(a+b)(a+b-1)} + 4 \cdot \frac{b(b-1)}{(a+b)(a+b-1)} \\ = \frac{4(a(a-1) + b(b-1))}{(a+b)(a+b-1)} 
$$

Now, substituting into the variance formula:

$$ 
\text{Var}(X) = \frac{4(a(a-1) + b(b-1))}{(a+b)(a+b-1)} - \left[\frac{2(a^2-a - b^2+b)}{(a+b)(a+b-1)}\right]^2 
$$

### B.2 (4)


```{r}
compute_expectation_X <- function(a, b) {
  (2 * (a^2-a-b^2+b)) / ((a + b) * (a + b - 1))
}

compute_variance_X <- function(a, b) {
  E_X <- compute_expectation_X(a, b)
  E_X2 <- (4 * (a * (a - 1) + b * (b - 1))) / ((a + b) * (a + b - 1))
  return(E_X2 - E_X^2)
}

```

### B.2 (5)

$$
E(\bar{X})=E\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n}\sum_{i=1}^{n}E(X_{i})\\=E(X)=\frac{2(a^2-a - b^2+b)}{(a+b)(a+b-1)}
$$


### B.2 (6)

The variance of the sample mean $\bar{X}$:

$$
Var(\bar{X}) = Var\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right) = \frac{1}{n^{2}}\sum_{i=1}^{n}Var(X_{i}) = \frac{Var(X)}{n}\\=\frac{1}{n} \cdot \left \{ \frac{4(a(a-1) + b(b-1))}{(a+b)(a+b-1)} - \left[\frac{2(a^2-a - b^2+b)}{(a+b)(a+b-1)}\right]^2 \right \} 
$$

### B.2 (7)

```{r}
p_X <- function(a, b) {
  p_2 <- a * (a - 1) / ((a + b) * (a + b - 1))
  p_0 <- 2 * (a * b) / ((a + b) * (a + b - 1))
  p_neg2 <- b * (b - 1) / ((a + b) * (a + b - 1))
  return(c(p_2, p_0, p_neg2))
}

sample_Xs <- function(a, b, n) {
  samples <- numeric(n)
  for (i in 1:n) {
    draw <- sample(c(2, 0, -2), 1, prob = p_X(a, b))
    samples[i] <- draw
  }
  return(samples)
}
```

### B.2 (8)

```{r}
# Set parameters
a <- 3
b <- 5
n <- 100000

E_X <- compute_expectation_X(a, b)
Var_X <- compute_variance_X(a, b)

num_simulations <- 100

sample_means <- numeric(num_simulations)
sample_variances <- numeric(num_simulations)

# 50 simulations
for (i in 1:num_simulations) {
  samples <- sample_Xs(a, b, n)
  sample_means[i] <- mean(samples)
  sample_variances[i] <- var(samples)
}

cat("E(X):", E_X, "\n")
cat("Var(X):", Var_X, "\n")

# par(mfrow = c(1, 2))

# mean dot plot
plot(sample_means, col = "blue", pch = 19, xlab = "Simulation Index",
     ylab = "Sample Mean", main = "Sample Means Simulations",
     ylim = range(c(sample_means, E_X)) + c(-0.06, 0.06)) 
abline(h = E_X, col = "red", lwd = 2, lty = 2) 
legend("topright", legend = "Theoretical Mean", col = "red", lty = 2, lwd = 2)

# variance dot plot
plot(sample_variances, col = "green", pch = 19, xlab = "Simulation Index",
     ylab = "Sample Variance", main = "Sample Variances Simulations",
     ylim = range(c(sample_variances, Var_X)) + c(-0.06, 0.06))
abline(h = Var_X, col = "red", lwd = 2, lty = 2)
legend("topright", legend = "Theoretical Variance", col = "red", lty = 2, lwd = 2)

mean_bias <- sample_means - E_X
variance_bias <- sample_variances - Var_X


mean_bias_summary <- list(
  MeanBias = mean(mean_bias),         
  MeanBiasSD = sd(mean_bias) 
)

variance_bias_summary <- list(
  VarianceBias = mean(variance_bias),
  VarianceBiasSD = sd(variance_bias)  
)

print(mean_bias_summary)
print(variance_bias_summary)

```

With a sample size of 100,000, I simulated 100 times and plotted the corresponding dot plots. Based on the simulated data, the average deviation between the sample mean $\bar{X}$ and the theoretical mean $E(X)$ is about 7e-06, and the average deviation between the sample variance and the theoretical variance $Var(X)$ is about 0.001, which are very very close.(**The deviations mentioned here will vary depending on the simulation results, and are only approximate**)

### B.2 (9)

```{r}
a <- 3
b <- 5
n <- 100
trials <- 50000

simulate_sample_means <- function(a, b, n, trials) {
  sample_means <- numeric(trials)
  
  for (i in 1:trials) {
    samples <- sample_Xs(a, b, n)
    sample_means[i] <- mean(samples)
  }
  
  return(sample_means)
}

sample_means <- simulate_sample_means(a, b, n, trials)

```

### B.2 (10)


```{r}
E_X <- compute_expectation_X(a, b)
Var_X <- compute_variance_X(a, b)
sigma <- sqrt(Var_X / n)

# Generate scatter values
x_values <- seq(E_X - 3 * sigma, E_X + 3 * sigma, by = 0.1)

# Calculate kernel density
normal_density <- dnorm(x_values, mean = E_X, sd = sigma)

sample_means <- simulate_sample_means(a, b, n, trials)

density_sample_means <- density(sample_means)

plot(density_sample_means, main = "Kernel Density of Sample Means with Scatter Points", xlab = "Value", ylab = "Density", col = "blue", lwd = 2)

lines(x_values, normal_density, col = "red", lwd = 2)

points(x_values, normal_density, col = "green", pch = 16)

legend("topright", legend = c("Kernel Density of Sample Means", "Normal Density", "Scatter Points"), col = c("blue", "red", "green"), pch = c(NA, NA, 16), lwd = c(2, 2, NA))


```

### B.2 (11)

- The kernel density estimates of the sample means (blue curves) are very close to the theoretical normally distributed densities (red curves), indicating that the distribution of the sample means tends to be normally distributed as the sample size increases, as expected from the central limit theorem.

- The scatter point (green point) shows the theoretical density value at this location, showing an overlap with the blue and red curves, further verifying the consistency between the kernel density estimate of the sample mean and the theoretical density.



